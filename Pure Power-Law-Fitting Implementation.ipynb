{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plfit\n",
    "import powerlaw\n",
    "from numpy.random import rand,seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krist\\Anaconda3\\lib\\site-packages\\plfit\\plfit.py:805: RuntimeWarning: invalid value encountered in power\n",
      "  Ppl = lambda X: 1+C*(xmin/(1-alpha)*(X/xmin)**(1-alpha))\n"
     ]
    }
   ],
   "source": [
    "X=plfit.plexp_inv(rand(5000),100,2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krist\\Anaconda3\\lib\\site-packages\\plfit\\plfit.py:118: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  a = 1+float(n) / sum(log(x/xmin))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHON plfit executed in 2.017143 seconds\n",
      "xmin: 102.42 n(>xmin): 639 alpha: 2.49887 +/- 0.0592943   Log-Likelihood: -3764.7   ks: 0.0148494 p(ks): 0.747052\n"
     ]
    }
   ],
   "source": [
    "myplfit=plfit.plfit(X,usefortran=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krist\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of subdatasets, where the power-law could not be fitted within the chosen tolerance level, are 0 and the total number of subdatasets are 1\n"
     ]
    }
   ],
   "source": [
    "pvalues, results, stats = getpowerlaw(X, times = 1, extend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alpha</th>\n",
       "      <td>2.49887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sigma</th>\n",
       "      <td>0.0592943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KS statistic</th>\n",
       "      <td>0.0148494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xmin</th>\n",
       "      <td>102.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#ofdatapoints</th>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "Alpha            2.49887\n",
       "Sigma          0.0592943\n",
       "KS statistic   0.0148494\n",
       "Xmin              102.42\n",
       "#ofdatapoints        639"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation uses the method of minimizing the distance between the empirical CDF and the CDF of the best-fit \n",
    "# model to determine the appropiate value of xmin. \n",
    "\n",
    "def getpowerlaw(data,tollevel = 0.1,times =2500,extend = False, validate = True): \n",
    "    \n",
    "    if isinstance(data,pd.DataFrame)==False:\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    notfitted = 0\n",
    "    \n",
    "    holder = pd.DataFrame(columns = data.columns,index = ['Alpha','Sigma','KS statistic','Xmin','#ofdatapoints'])\n",
    "    p_values = []\n",
    "    simstats = []\n",
    "    for column in data:\n",
    "        \n",
    "        # The printing can be useful to understand where in the fitting process the algorithm are.\n",
    "        \n",
    "        #print('It is dataset number %i that is being processed' % int(column))\n",
    "        \n",
    "        columndata = data[column]\n",
    "        columndata = columndata.sort_values()\n",
    "        \n",
    "        if extend:\n",
    "            \n",
    "            ksvalues,sigmas,alphas,xmins,datapoints,datatype = modelfitter(columndata, tollevel, extend = True)\n",
    "             \n",
    "        else:\n",
    "            \n",
    "            ksvalues,datatype = modelfitter(columndata, tollevel, extend = False)\n",
    "        \n",
    "        # Locating the specifications that caused the minimum distance between the empirical and theoretical distribution.\n",
    "        if len(ksvalues)==0:\n",
    "            \n",
    "            # Public-service announcement\n",
    "            \n",
    "            #print('The power-law can not be fitted within the tolerance level chosen')\n",
    "            notfitted += 1\n",
    "            continue\n",
    "            \n",
    "        bestks = np.argmin(ksvalues)\n",
    "        \n",
    "        # If no tail a present, the data can't be taught to follow a power-law distribution\n",
    "        if xmins[bestks]==data[column].min():\n",
    "            continue\n",
    "\n",
    "        holder[column]['Alpha'] = alphas[bestks]\n",
    "        holder[column]['Sigma'] = sigmas[bestks]\n",
    "        holder[column]['KS statistic'] = ksvalues[bestks]\n",
    "        holder[column]['Xmin'] = xmins[bestks]\n",
    "        holder[column]['#ofdatapoints'] = datapoints[bestks]\n",
    "\n",
    "        # The user has the opportunity to get the data validated, i.e. that it actually comes from a power law distribution.\n",
    "        # We know go on and use the best-fit specifications to validate if our data indeed does follow a power-law.\n",
    "        \n",
    "        if validate == True:\n",
    "\n",
    "            if extend:\n",
    "                nonpowerlawdata = columndata[columndata<xmins[bestks]]\n",
    "                p_val, simstat = validatepowerlaw(alphas[bestks], xmins[bestks], ksvalues[bestks], len(columndata),\n",
    "                                                   datapoints[bestks],columndata[columndata<xmins[bestks]],\n",
    "                                                   times, datatype, tollevel, extend = extend)\n",
    "            else:\n",
    "                nonpowerlawdata = columndata[columndata<xmins[bestks]]\n",
    "                p_val = validatepowerlaw(alphas[bestks], xmins[bestks], ksvalues[bestks], len(columndata),datapoints[bestks],\n",
    "                                     columndata[columndata<xmins[bestks]], times, datatype, tollevel, extend = extend)\n",
    "            p_values.append(p_val)\n",
    "            simstats.append(simstat)\n",
    "    print('The number of subdatasets, where the power-law could not be fitted within the chosen tolerance level, are %i and the total number of subdatasets are %i' % (notfitted,len(data.columns)))\n",
    "    \n",
    "    if notfitted == len(data.columns):\n",
    "        return 0,0,0\n",
    "    \n",
    "    if extend == True and validate == True:\n",
    "\n",
    "        return p_values,holder,simstats\n",
    "\n",
    "    elif extend == False and validate == True:\n",
    "\n",
    "        return p_values,holder\n",
    "        \n",
    "    elif extend == False and validate == False:\n",
    "\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validatepowerlaw(alpha,xmin,bestks,lenoforiginaldata,lenoftaildata,data,times = 2500,\n",
    "                     datatype = 'Discrete',tollevel=0.1,extend = False):\n",
    "    p_value = 0\n",
    "    \n",
    "    a, x = alpha,xmin\n",
    "    N = lenoforiginaldata\n",
    "    N_xmin = lenoftaildata\n",
    "    prob1 = N_xmin/N\n",
    "    prob2 = 1-prob1\n",
    "    \n",
    "    # Drawing the synthetic data that is needed to validate the hypothesis about our data following a power-law distribution.\n",
    "    \n",
    "    samples = samplesyntheticdata(data,N,prob1,prob2,times,a,x,datatype)\n",
    "    \n",
    "    # If the user wants information on each of the simulation, he shall get it - it follows down the code.\n",
    "    \n",
    "    if extend:\n",
    "        simulatedstats = pd.DataFrame(columns = samples.columns,\n",
    "                                      index = ['Alpha','Sigma','KS statistic','Xmin','#ofdatapoints'])\n",
    "    \n",
    "    # For each sample, we fit over power law and calculate the KS statistic.\n",
    "    \n",
    "    for sample in samples:\n",
    "        \n",
    "        cursample = samples[sample].sort_values()\n",
    "        \n",
    "        if extend:\n",
    "            # The part that goes out of the area: modelfitter(cursample, tollevel,extend = True)\n",
    "            ksvalues_sample,sigmas_sample,alphas_sample,xmins_sample,datapoints_sample, datatype = modelfitter(cursample, tollevel,\n",
    "                                                                                                               extend = True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ksvalues_sample,datatype = modelfitter(cursample, extend = False)\n",
    "       \n",
    "        # If no test statistic could be calculated, a new sample is drawn until a test statistic is calculated.\n",
    "        if len(ksvalues_sample) == 0:\n",
    "            holdme = sample\n",
    "            j = 0\n",
    "            \n",
    "            while(len(ksvalues_sample)==0):\n",
    "                \n",
    "                # The printing can be useful to understand the dynamics of the fitting.\n",
    "                \n",
    "                #print('It is the %i iteration in the request of a sample which is useful' % (j+1))\n",
    "                \n",
    "                sample = samplesyntheticdata(data,N,prob1,prob2,1,a,x,datatype)\n",
    "                \n",
    "                cursample = sample[sample.columns[0]].sort_values()\n",
    "                \n",
    "                if extend:\n",
    "                    # The part that goes out of the area: modelfitter(cursample, tollevel,extend = True)\n",
    "                    ksvalues_sample,sigmas_sample,alphas_sample,xmins_sample,datapoints_sample, datatype = modelfitter(cursample, tollevel,\n",
    "                                                                                                                       extend = True)\n",
    "            \n",
    "                else:\n",
    "            \n",
    "                    ksvalues_sample, datatype = modelfitter(cursample, extend = False)\n",
    "                        \n",
    "                j += 1\n",
    "            \n",
    "            sample = holdme\n",
    "            \n",
    "            # The printing can be useful to understand the dynamics of the fitting.\n",
    "            \n",
    "            #print('It took %i iterations to get a sample which was useful' % (j+1))\n",
    "                    \n",
    "        bestks_sample = np.argmin(ksvalues_sample)\n",
    "        \n",
    "        if extend:\n",
    "            \n",
    "            simulatedstats[sample]['Alpha'] = alphas_sample[bestks_sample]\n",
    "            simulatedstats[sample]['Sigma'] = sigmas_sample[bestks_sample]\n",
    "            simulatedstats[sample]['KS statistic'] = ksvalues_sample[bestks_sample]\n",
    "            simulatedstats[sample]['Xmin'] = xmins_sample[bestks_sample]\n",
    "            simulatedstats[sample]['#ofdatapoints'] = datapoints_sample[bestks_sample]\n",
    "        \n",
    "        if ksvalues_sample[bestks_sample]>bestks:\n",
    "            \n",
    "            p_value += 1.0\n",
    "    \n",
    "    if extend:\n",
    "        \n",
    "        return p_value/float(times),simulatedstats\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return p_value/float(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplesyntheticdata(data,lenoforiginaldata,prob1,prob2,times,alpha,xmin,datatype):\n",
    "    \n",
    "    # Firstly, we draw all the necessary draws from the left-out observations\n",
    "    \n",
    "    sample_1 = pd.DataFrame(np.random.choice(data,(round(prob2*lenoforiginaldata),times)))\n",
    "    \n",
    "    #Initiallizing the dataframe to contain the total amount of draws\n",
    "    \n",
    "    samples_2 = pd.DataFrame()\n",
    "    \n",
    "    # Drawing our synthetic data, to be used for getting the p-values for the hypothesis of if our data comes from \n",
    "    # a power-law distribution.\n",
    "    if times > 1:\n",
    "        \n",
    "        for t in np.arange(times):\n",
    "\n",
    "            samples_2[t] = powerlaw.Power_Law(xmin=xmin, parameters=[alpha]).generate_random(round(prob1*lenoforiginaldata))\n",
    "    else:\n",
    "        \n",
    "        samples_2[0] = powerlaw.Power_Law(xmin=xmin, parameters=[alpha]).generate_random(round(prob1*lenoforiginaldata))\n",
    "    \n",
    "    samples_3=pd.concat([samples_2,sample_1],ignore_index=True)\n",
    "    \n",
    "    # If the data is discrete data, we round of the data, consistent with what is proposed in [Clauset et al., 2009]\n",
    "    \n",
    "    if datatype == 'Discrete':\n",
    "        samples_3 = samples_3.round()\n",
    "    \n",
    "    return samples_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfitter(data, tollevel = 0.1,extend = False):\n",
    "\n",
    "    ksvalues = []\n",
    "        \n",
    "    if extend:\n",
    "\n",
    "        sigmas = []\n",
    "        xmins = []\n",
    "        alphas = []\n",
    "        datapoints = []\n",
    "\n",
    "    for point in np.unique(data):\n",
    "        \n",
    "        # If the xmin candicate is zero, the calculations breaks down.\n",
    "        if point == 0:\n",
    "            continue\n",
    "\n",
    "        currentdata = data[data >=point]\n",
    "        n = len(currentdata)\n",
    "\n",
    "        # Chosing the appropiate estimator for alpha\n",
    "\n",
    "        if (data.min()/int(data.min()) == 1 and data.median()/int(data.median())==1):\n",
    "\n",
    "            datatype = 'Discrete'\n",
    "\n",
    "            # Approximate alpha expression based on the assumption that integers are approximated as\n",
    "            # continous reals rounded to the nearest integer. (Formula 3.7)\n",
    "\n",
    "            alpha = (1+n*(sum(np.log(currentdata/(point-0.5)))**(-1)))\n",
    "\n",
    "        else:\n",
    "\n",
    "            datatype = 'Continuous'\n",
    "            alpha = (1+n*(sum(np.log(currentdata/point))**(-1)))\n",
    "\n",
    "        # The estimator for Alpha is asymptotic normal distributed and consistent.\n",
    "        # Therefore it is possible to evaluate the sigma of the estimator based on the normal p-values.\n",
    "\n",
    "        # Making a conservative adjustment if the sample size is particular small\n",
    "\n",
    "        if len(currentdata)<50:\n",
    "\n",
    "            sigma = (alpha - 1)/np.sqrt(n) + (1/n)\n",
    "\n",
    "        else:\n",
    "\n",
    "            sigma = (alpha - 1)/np.sqrt(n)\n",
    "\n",
    "        # We disregard estimates of alpha that results in a higher standard error that 0.1.\n",
    "        # The reason here of is fact that we drop observations every time we examin a new xmin, and\n",
    "        # we achknowledge that we are working few observations, so a higher tolerance level is accpeted..\n",
    "        \n",
    "        if sigma < tollevel:\n",
    "\n",
    "            cdfres = np.arange(len(currentdata))/len(currentdata)\n",
    "\n",
    "            # Evaluation of the Complimentary Cumulative distribution function in calculation of the KS statistic\n",
    "\n",
    "            bestfitpowerCDF = 1-(currentdata.sort_values()/point)**(-alpha+1)\n",
    "\n",
    "            ksvalues.append(max(abs(bestfitpowerCDF-cdfres)))\n",
    "\n",
    "            if extend:\n",
    "\n",
    "                sigmas.append(sigma)\n",
    "                alphas.append(alpha)\n",
    "                xmins.append(point)\n",
    "                datapoints.append(n)\n",
    "\n",
    "        else:\n",
    "\n",
    "            break\n",
    "            \n",
    "    if extend:\n",
    "\n",
    "        return ksvalues,sigmas,alphas,xmins,datapoints,datatype\n",
    "\n",
    "    else:\n",
    "\n",
    "        return ksvalues,datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate power-law distributed artificial data, to test the above inplementation\n",
    "\n",
    "alpha, xmin = 2.5, 100\n",
    "\n",
    "# These probabilities are for chosen arbitrary\n",
    "\n",
    "# Uncomment this if you want to sample some of the distribution from a distribution of your own choice.\n",
    "# Furthermore, uncomment the two outcommented lines below, one in the for-loop and one right after.\n",
    "\n",
    "#tail = pd.DataFrame()\n",
    "#neck = pd.DataFrame(C*np.random.exponential(lamb,(round(prob2*N),times)))\n",
    "#prob1 = 0.8\n",
    "#prob2 = 1-prob1\n",
    "#lamb = 0.5\n",
    "#C = np.exp(lamb*xmin)\n",
    "\n",
    "N = 2500\n",
    "times = 5000\n",
    "\n",
    "artificialdata = pd.DataFrame()\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for gen in np.arange(times):\n",
    "        \n",
    "        #tail[gen] = powerlaw.Power_Law(xmin=xmin, parameters=[alpha]).generate_random(round(prob1*N))\n",
    "        artificialdata[gen] = plfit.plexp_inv(rand(N),xmin,alpha)\n",
    "\n",
    "# If you need continuous data, just remove/outcomment the rounding.\n",
    "\n",
    "#artificialdata = round(pd.concat([tail,neck],ignore_index=True))\n",
    "artificialdata = round(artificialdata)        \n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('It took %.3f to generate %i synthetic dataset of %i observations' % ((end-start),times,N))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
